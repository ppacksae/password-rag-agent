import streamlit as st
import google.generativeai as genai
import PyPDF2
from docx import Document
import io
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¢ AHN's AI ë„ìš°ë¯¸",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì œëª©
st.title("ğŸ¢ AHN's AI ë„ìš°ë¯¸")
st.write("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  AIì™€ ëŒ€í™”í•˜ë©° ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ì…ë ¥
    api_key = st.text_input("Google Gemini API í‚¤:", type="password")
    
    if api_key:
        genai.configure(api_key=api_key)
        st.success("âœ… API í‚¤ ì„¤ì • ì™„ë£Œ!")
    
    st.divider()
    
    # ë¬¸ì„œ ì—…ë¡œë“œ
    st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf', 'docx', 'txt'],
        accept_multiple_files=True
    )

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'documents' not in st.session_state:
    st.session_state.documents = []

if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

if 'encoder' not in st.session_state:
    st.session_state.encoder = None

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def extract_text_from_pdf(file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
        return ""

def extract_text_from_docx(file):
    """DOCXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = Document(io.BytesIO(file.read()))
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        st.error(f"DOCX ì½ê¸° ì˜¤ë¥˜: {e}")
        return ""

def extract_text_from_txt(file):
    """TXTì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return file.read().decode('utf-8')
    except Exception as e:
        st.error(f"TXT ì½ê¸° ì˜¤ë¥˜: {e}")
        return ""

def split_text_into_chunks(text, chunk_size=500):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not text.strip():
        return []
    
    sentences = text.replace('\n', ' ').split('. ')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_length = len(sentence)
        
        if current_length + sentence_length > chunk_size and current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 50]  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œê±°

def process_documents(files):
    """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ ì²˜ë¦¬"""
    documents = []
    
    for file in files:
        try:
            # íŒŒì¼ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if file.type == "application/pdf":
                text = extract_text_from_pdf(file)
            elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                text = extract_text_from_docx(file)
            elif file.type == "text/plain":
                text = extract_text_from_txt(file)
            else:
                st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file.name}")
                continue
            
            if not text.strip():
                st.warning(f"íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file.name}")
                continue
            
            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = split_text_into_chunks(text, chunk_size=500)
            
            for i, chunk in enumerate(chunks):
                documents.append({
                    'id': f"{file.name}_{i}",
                    'text': chunk,
                    'filename': file.name,
                    'chunk_id': i
                })
        
        except Exception as e:
            st.error(f"íŒŒì¼ {file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return documents

@st.cache_resource
def load_sentence_transformer():
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    try:
        return SentenceTransformer('all-MiniLM-L6-v2')
    except Exception as e:
        st.error(f"SentenceTransformer ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def create_embeddings(documents):
    """ë¬¸ì„œ ì„ë² ë”© ìƒì„±"""
    if not documents:
        return None
    
    try:
        encoder = load_sentence_transformer()
        if encoder is None:
            return None
        
        texts = [doc['text'] for doc in documents]
        embeddings = encoder.encode(texts)
        
        return embeddings, encoder
    
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None

def search_documents(query, documents, embeddings, encoder, n_results=3):
    """ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    try:
        if not documents or embeddings is None or encoder is None:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = encoder.encode([query])
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, embeddings)[0]
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:n_results]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                results.append(documents[idx]['text'])
        
        return results
    
    except Exception as e:
        st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def generate_response(query, context_docs, api_key):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context_docs:
            context = "\n\n".join(context_docs)
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ì‹œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
"""
        else:
            prompt = f"""
ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ë‹µë³€ì„ ì œê³µí•˜ë˜, 
"ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ë“œë¦½ë‹ˆë‹¤"ë¼ê³  ë¨¼ì € ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
"""
        
        response = model.generate_content(prompt)
        return response.text
    
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ’¬ AI ì±„íŒ…")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        if api_key:
            with st.chat_message("assistant"):
                with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    # ë¬¸ì„œ ê²€ìƒ‰
                    relevant_docs = search_documents(
                        prompt, 
                        st.session_state.documents, 
                        st.session_state.embeddings, 
                        st.session_state.encoder
                    )
                    
                    # ì‘ë‹µ ìƒì„±
                    response = generate_response(prompt, relevant_docs, api_key)
                    
                    st.markdown(response)
                    
                    # ì°¾ì€ ë¬¸ì„œ ì •ë³´ í‘œì‹œ
                    if relevant_docs:
                        with st.expander(f"ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ({len(relevant_docs)}ê°œ)"):
                            for i, doc in enumerate(relevant_docs):
                                st.write(f"**ë¬¸ì„œ {i+1}:**")
                                st.write(doc[:200] + "..." if len(doc) > 200 else doc)
                                st.divider()
                    
                    # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥
                    st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            with st.chat_message("assistant"):
                st.error("API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")

with col2:
    st.header("ğŸ“Š ë¬¸ì„œ í˜„í™©")
    
    # ë¬¸ì„œ ì—…ë¡œë“œ ì²˜ë¦¬
    if uploaded_files:
        if st.button("ğŸ“¤ ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°", type="primary"):
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ë¬¸ì„œ ì²˜ë¦¬
                documents = process_documents(uploaded_files)
                
                if documents:
                    st.session_state.documents = documents
                    
                    # ì„ë² ë”© ìƒì„±
                    with st.spinner("ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        embeddings, encoder = create_embeddings(documents)
                        if embeddings is not None:
                            st.session_state.embeddings = embeddings
                            st.session_state.encoder = encoder
                            st.success(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            st.error("âŒ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("âš ï¸ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                st.rerun()
    
    # í˜„ì¬ ë¬¸ì„œ ìƒíƒœ í‘œì‹œ
    if st.session_state.documents:
        st.info(f"ğŸ“ ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(st.session_state.documents)}ê°œ ì²­í¬")
        
        # íŒŒì¼ë³„ ì²­í¬ ìˆ˜ í‘œì‹œ
        file_counts = {}
        for doc in st.session_state.documents:
            filename = doc['filename']
            file_counts[filename] = file_counts.get(filename, 0) + 1
        
        for filename, count in file_counts.items():
            st.write(f"â€¢ {filename}: {count}ê°œ ì²­í¬")
        
        # ì„ë² ë”© ìƒíƒœ
        if st.session_state.embeddings is not None:
            st.success("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í™œì„±í™”ë¨")
        else:
            st.warning("âš ï¸ ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    st.header("ğŸ—‘ï¸ ê´€ë¦¬")
    
    col_clear1, col_clear2 = st.columns(2)
    
    with col_clear1:
        if st.button("ğŸ”„ ì±„íŒ… ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.rerun()
    
    with col_clear2:
        if st.button("ğŸ“‚ ë¬¸ì„œ ì´ˆê¸°í™”"):
            st.session_state.documents = []
            st.session_state.embeddings = None
            st.session_state.encoder = None
            st.rerun()

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("ğŸ“– ì‚¬ìš©ë²• ì•ˆë‚´"):
    st.markdown("""
    ### ğŸš€ ì‹œì‘í•˜ê¸°
    1. **ì‚¬ì´ë“œë°”**ì—ì„œ Google Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
    2. **ë¬¸ì„œë¥¼ ì—…ë¡œë“œ**í•˜ê³  "ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    3. **ì±„íŒ…ì°½**ì—ì„œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”
    
    ### ğŸ’¡ íŒ
    - PDF, DOCX, TXT íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤
    - ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ë‹µë³€ í•˜ë‹¨ì˜ "ì°¸ê³ í•œ ë¬¸ì„œ" ì„¹ì…˜ì—ì„œ ì¶œì²˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    
    ### âš ï¸ ì£¼ì˜ì‚¬í•­
    - API í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ì„¸ìš”
    - ì—…ë¡œë“œëœ ë¬¸ì„œëŠ” ì„¸ì…˜ì´ ëë‚˜ë©´ ì‚­ì œë©ë‹ˆë‹¤
    - ì²« ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)

# í‘¸í„°
st.divider()
st.markdown("**ğŸ¤– Google Gemini ê¸°ë°˜ RAG ì±—ë´‡** | ë¬¸ì„œ ê¸°ë°˜ ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
