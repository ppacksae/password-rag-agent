import streamlit as st
import google.generativeai as genai
import PyPDF2
from docx import Document
import io
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Corporate AI Assistant",
    page_icon="ğŸ’¼",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ê¹”ë”í•œ í™”ì´íŠ¸ ëª¨ë“œ CSS
st.markdown("""
<style>
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 120px;
        padding-left: 2rem;
        padding-right: 2rem;
        max-width: none;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .css-1d391kg {
        background-color: #f8f9fa;
        border-right: 1px solid #e9ecef;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1 {
        color: #2c3e50;
        font-weight: 700;
        margin-bottom: 0.5rem;
    }
    
    h2, h3 {
        color: #34495e;
        font-weight: 600;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background-color: #3498db;
        color: white;
        border: none;
        border-radius: 6px;
        font-weight: 500;
        padding: 0.6rem 1.2rem;
        transition: all 0.2s ease;
        box-shadow: 0 2px 4px rgba(52, 152, 219, 0.2);
    }
    
    .stButton > button:hover {
        background-color: #2980b9;
        transform: translateY(-1px);
        box-shadow: 0 4px 8px rgba(52, 152, 219, 0.3);
    }
    
    /* Primary ë²„íŠ¼ */
    .stButton > button[kind="primary"] {
        background-color: #27ae60;
        box-shadow: 0 2px 4px rgba(39, 174, 96, 0.2);
    }
    
    .stButton > button[kind="primary"]:hover {
        background-color: #229954;
        box-shadow: 0 4px 8px rgba(39, 174, 96, 0.3);
    }
    
    /* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ */
    .stTextInput > div > div > input {
        border: 2px solid #e9ecef;
        border-radius: 6px;
        padding: 0.6rem;
        transition: border-color 0.2s ease;
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #3498db;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1);
    }
    
    /* íŒŒì¼ ì—…ë¡œë” ìŠ¤íƒ€ì¼ */
    .stFileUploader {
        border: 2px dashed #bdc3c7;
        border-radius: 8px;
        padding: 1.5rem;
        background-color: #f8f9fa;
        transition: all 0.2s ease;
    }
    
    .stFileUploader:hover {
        border-color: #3498db;
        background-color: #ecf0f1;
    }
    
    /* ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stSuccess {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        border-radius: 6px;
    }
    
    .stInfo {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
        border-radius: 6px;
    }
    
    .stWarning {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        border-radius: 6px;
    }
    
    .stError {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        border-radius: 6px;
    }
    
    /* í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ */
    .streamlit-expanderHeader {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 6px;
        font-weight: 500;
    }
    
    .streamlit-expanderContent {
        border: 1px solid #e9ecef;
        border-top: none;
        border-radius: 0 0 6px 6px;
        background-color: #ffffff;
    }
    
    /* ë©”íŠ¸ë¦­ ì»¨í…Œì´ë„ˆ */
    [data-testid="metric-container"] {
        background-color: #ffffff;
        border: 1px solid #e9ecef;
        border-radius: 6px;
        padding: 1rem;
        box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
    }
    
    /* êµ¬ë¶„ì„  */
    hr {
        border-color: #e9ecef;
        margin: 1.5rem 0;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ */
    [data-testid="stChatInput"] textarea {
        border: 2px solid #e9ecef;
        border-radius: 25px;
        padding: 12px 20px;
        font-size: 1rem;
        transition: border-color 0.2s ease;
    }
    
    [data-testid="stChatInput"] textarea:focus {
        border-color: #3498db;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1);
    }
    
    [data-testid="stChatInput"] textarea::placeholder {
        color: #7f8c8d;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ìœ„ì¹˜ */
    .stChatInput {
        position: fixed;
        bottom: 0;
        left: 320px;
        right: 0;
        background: white;
        border-top: 1px solid #e9ecef;
        padding: 1rem 2rem;
        z-index: 999;
        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ì´ë“œë°” ì¶•ì†Œì‹œ */
    @media (max-width: 768px) {
        .stChatInput {
            left: 0;
            padding: 1rem;
        }
        
        .main .block-container {
            padding-left: 1rem;
            padding-right: 1rem;
        }
    }
    
    /* ë§í¬ ìƒ‰ìƒ */
    a {
        color: #3498db;
        text-decoration: none;
    }
    
    a:hover {
        color: #2980b9;
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

# ì œëª© ë° í—¤ë”
st.title("AHN'S AI Assistant")
st.markdown("**Enterprise Document Intelligence Platform**")
st.markdown("---")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'documents' not in st.session_state:
    st.session_state.documents = []

if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

if 'encoder' not in st.session_state:
    st.session_state.encoder = None

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'documents' not in st.session_state:
    st.session_state.documents = []

if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

if 'encoder' not in st.session_state:
    st.session_state.encoder = None

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def extract_text_from_pdf(file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF reading error: {e}")
        return ""

def extract_text_from_docx(file):
    """DOCXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = Document(io.BytesIO(file.read()))
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        st.error(f"DOCX reading error: {e}")
        return ""

def extract_text_from_txt(file):
    """TXTì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return file.read().decode('utf-8')
    except Exception as e:
        st.error(f"TXT reading error: {e}")
        return ""

def split_text_into_chunks(text, chunk_size=500):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not text.strip():
        return []
    
    sentences = text.replace('\n', ' ').split('. ')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_length = len(sentence)
        
        if current_length + sentence_length > chunk_size and current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 50]

def process_documents(files):
    """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ ì²˜ë¦¬"""
    documents = []
    
    for file in files:
        try:
            if file.type == "application/pdf":
                text = extract_text_from_pdf(file)
            elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                text = extract_text_from_docx(file)
            elif file.type == "text/plain":
                text = extract_text_from_txt(file)
            else:
                st.warning(f"Unsupported file format: {file.name}")
                continue
            
            if not text.strip():
                st.warning(f"No text extracted from: {file.name}")
                continue
            
            chunks = split_text_into_chunks(text, chunk_size=500)
            
            for i, chunk in enumerate(chunks):
                documents.append({
                    'id': f"{file.name}_{i}",
                    'text': chunk,
                    'filename': file.name,
                    'chunk_id': i
                })
        
        except Exception as e:
            st.error(f"Error processing {file.name}: {e}")
    
    return documents

@st.cache_resource
def load_sentence_transformer():
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
    try:
        return SentenceTransformer('all-MiniLM-L6-v2')
    except Exception as e:
        st.error(f"SentenceTransformer loading error: {e}")
        return None

def create_embeddings(documents):
    """ë¬¸ì„œ ì„ë² ë”© ìƒì„±"""
    if not documents:
        return None
    
    try:
        encoder = load_sentence_transformer()
        if encoder is None:
            return None
        
        texts = [doc['text'] for doc in documents]
        embeddings = encoder.encode(texts)
        
        return embeddings, encoder
    
    except Exception as e:
        st.error(f"Embedding generation error: {e}")
        return None, None

def search_documents(query, documents, embeddings, encoder, n_results=3):
    """ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    try:
        if not documents or embeddings is None or encoder is None:
            return []
        
        query_embedding = encoder.encode([query])
        similarities = cosine_similarity(query_embedding, embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:n_results]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:
                results.append(documents[idx]['text'])
        
        return results
    
    except Exception as e:
        st.error(f"Document search error: {e}")
        return []

def generate_response(query, context_docs, api_key):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        if context_docs:
            context = "\n\n".join(context_docs)
            prompt = f"""
Based on the following document content, please answer the question professionally.

Document Content:
{context}

Question: {query}

Please follow these guidelines:
1. Answer accurately based on the document content
2. If information is not in the documents, state "The requested information is not available in the uploaded documents"
3. Use a professional and helpful tone
4. Include specific examples or details when possible
5. Respond in Korean if the question is in Korean
"""
        else:
            prompt = f"""
No uploaded documents found or no relevant information available.

Question: {query}

Please provide a general response based on your knowledge, but first mention that "No relevant information was found in the uploaded documents, so I'm providing a general response."
Respond in Korean if the question is in Korean.
"""
        
        response = model.generate_content(prompt)
        return response.text
    
    except Exception as e:
        return f"Error generating response: {e}"

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("Configuration")
    
    # API í‚¤ ì…ë ¥
    api_key = st.text_input("Google Gemini API Key:", type="password", help="Enter your API key to enable AI features")
    
    if api_key:
        genai.configure(api_key=api_key)
        st.success("API Connected")
    else:
        st.warning("API Key Required")
    
    st.markdown("---")
    
    # ë¬¸ì„œ ê´€ë¦¬ ì„¹ì…˜
    st.header("Document Management")
    
    # ë¬¸ì„œ ì—…ë¡œë“œ
    st.subheader("Upload Documents")
    uploaded_files = st.file_uploader(
        "Supported formats: PDF, DOCX, TXT",
        type=['pdf', 'docx', 'txt'],
        accept_multiple_files=True,
        help="Upload company documents for AI analysis"
    )
    
    # ë¬¸ì„œ ì²˜ë¦¬ ë²„íŠ¼
    if uploaded_files:
        if st.button("Process Documents", type="primary", use_container_width=True):
            with st.spinner("Processing documents..."):
                # ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§
                documents = process_documents(uploaded_files)
                
                if documents:
                    st.session_state.documents = documents
                    
                    with st.spinner("Generating embeddings..."):
                        embeddings, encoder = create_embeddings(documents)
                        if embeddings is not None:
                            st.session_state.embeddings = embeddings
                            st.session_state.encoder = encoder
                            st.success(f"Processed {len(documents)} document chunks")
                        else:
                            st.error("Embedding generation failed")
                else:
                    st.warning("No processable documents found")
                
                st.rerun()
    
    st.markdown("---")
    
    # ë¬¸ì„œ í˜„í™©
    st.subheader("Document Status")
    if st.session_state.get('documents'):
        st.metric("Total Chunks", len(st.session_state.documents))
        
        # íŒŒì¼ë³„ ì²­í¬ ìˆ˜ í‘œì‹œ
        file_counts = {}
        for doc in st.session_state.documents:
            filename = doc['filename']
            file_counts[filename] = file_counts.get(filename, 0) + 1
        
        for filename, count in file_counts.items():
            st.text(f"{filename}: {count} chunks")
        
        # ê²€ìƒ‰ ê¸°ëŠ¥ ìƒíƒœ
        if st.session_state.get('embeddings') is not None:
            st.success("Search: Active")
        else:
            st.warning("Search: Inactive")
    else:
        st.info("No documents loaded")
    
    st.markdown("---")
    
    # ê´€ë¦¬ ê¸°ëŠ¥
    st.subheader("System Management")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("Clear Docs", use_container_width=True):
            st.session_state.documents = []
            st.session_state.embeddings = None
            st.session_state.encoder = None
            st.rerun()

# ì‚¬ìš©ë²• ì•ˆë‚´ë¥¼ ì‚¬ì´ë“œë°”ë¡œ ì´ë™
with st.sidebar:
    st.markdown("---")
    st.subheader("System Information")
    
    with st.expander("Getting Started"):
        st.markdown("""
        1. Enter your Google Gemini API key above
        2. Upload documents using the file uploader
        3. Click "Process Documents" to enable AI search
        4. Ask questions about your documents in the chat
        """)
    
    with st.expander("Features"):
        st.markdown("""
        - PDF, DOCX, TXT file support
        - Multiple file upload capability
        - Vector-based document search
        - Professional AI responses
        - Source document references
        """)
    
    with st.expander("Tips"):
        st.markdown("""
        - Use specific questions for better results
        - Multiple files can be processed together
        - First document processing may take time
        - Referenced sources shown below responses
        """)

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.header("AI Chat Interface")

# ì±„íŒ… ì»¨í…Œì´ë„ˆ (ì‚¬ì´ë“œë°” ë„ˆë¹„ë§Œí¼ ì—¬ë°± ì¶”ê°€)
chat_container = st.container()

with chat_container:
    # ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (ì±„íŒ…ì´ ë¹„ì–´ìˆì„ ë•Œë§Œ í‘œì‹œ)
    if not st.session_state.messages:
        st.markdown("""
        <div style="
            display: flex;
            justify-content: center;
            align-items: center;
            height: 300px;
            flex-direction: column;
            margin-left: 0;
        ">
            <h1 style="
                font-size: 3rem;
                font-weight: 300;
                color: #2c3e50;
                margin-bottom: 2rem;
                text-align: center;
            ">ì•ˆë…•í•˜ì„¸ìš”</h1>
            <p style="
                font-size: 1.2rem;
                color: #7f8c8d;
                text-align: center;
                margin-bottom: 3rem;
            ">AHN'S AI Assistantê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤</p>
        </div>
        """, unsafe_allow_html=True)

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ (ì»¤ìŠ¤í…€ ìŠ¤íƒ€ì¼)
    if st.session_state.messages:
        for i, message in enumerate(st.session_state.messages):
            if message["role"] == "user":
                # ì‚¬ìš©ì ë©”ì‹œì§€ - ìš°ì¸¡ ì •ë ¬
                st.markdown(f"""
                <div style="
                    display: flex;
                    justify-content: flex-end;
                    margin: 1rem 0;
                    padding-right: 1rem;
                ">
                    <div style="
                        background-color: #e3f2fd;
                        color: #1565c0;
                        padding: 0.8rem 1.2rem;
                        border-radius: 18px 18px 4px 18px;
                        max-width: 70%;
                        font-size: 0.95rem;
                        line-height: 1.4;
                        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
                        word-wrap: break-word;
                    ">
                        {message["content"]}
                    </div>
                </div>
                """, unsafe_allow_html=True)
            else:
                # AI ì‘ë‹µ - ì¢Œì¸¡ ì •ë ¬
                # HTML íƒœê·¸ ì œê±°í•˜ê³  ê¹”ë”í•˜ê²Œ í‘œì‹œ
                clean_content = message["content"].replace('<div>', '').replace('</div>', '').strip()
                
                st.markdown(f"""
                <div style="
                    display: flex;
                    justify-content: flex-start;
                    margin: 1rem 0;
                    align-items: flex-start;
                    padding-left: 1rem;
                ">
                    <div style="
                        background-color: #f5f5f5;
                        color: #2c3e50;
                        padding: 0.8rem 1.2rem;
                        border-radius: 18px 18px 18px 4px;
                        max-width: 75%;
                        font-size: 0.95rem;
                        line-height: 1.5;
                        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
                        border: 1px solid #e9ecef;
                        word-wrap: break-word;
                        white-space: pre-wrap;
                    ">
                        {clean_content}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # ì°¸ê³  ë¬¸ì„œê°€ ìˆì„ ê²½ìš° í‘œì‹œ
                if "references" in message:
                    with st.expander(f"ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ({len(message['references'])}ê°œ)"):
                        for j, doc in enumerate(message["references"]):
                            st.write(f"**ë¬¸ì„œ {j+1}:**")
                            st.write(doc[:200] + "..." if len(doc) > 200 else doc)
                            if j < len(message["references"]) - 1:
                                st.markdown("---")

# ì»¤ìŠ¤í…€ ì±„íŒ… ì…ë ¥ì°½
st.markdown("""
<style>
    /* ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ì‚¬ì´ë“œë°” ê²¹ì¹¨ ë°©ì§€ */
    .main .block-container {
        padding-bottom: 120px !important;
        padding-left: 1rem !important;
        padding-right: 1rem !important;
    }
    
    /* ì‚¬ì´ë“œë°”ê°€ ìˆì„ ë•Œ ë©”ì¸ ì½˜í…ì¸  ì—¬ë°± ì¡°ì • */
    .main {
        margin-left: 0 !important;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        margin-left: 0;
        width: 100%;
        max-width: none;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ì»¤ìŠ¤í„°ë§ˆì´ì§• */
    .stChatInput {
        position: fixed;
        bottom: 0;
        left: 320px; /* ì‚¬ì´ë“œë°” ë„ˆë¹„ë§Œí¼ ì—¬ë°± */
        right: 0;
        background: white;
        border-top: 1px solid #e9ecef;
        padding: 1rem;
        z-index: 999;
        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ì´ë“œë°”ê°€ ì¶•ì†Œëœ ê²½ìš° */
    .css-1lcbmhc.e1fqkh3o0 + .main .stChatInput {
        left: 60px;
    }
    
    [data-testid="stChatInput"] {
        margin-bottom: 0;
        max-width: calc(100vw - 360px); /* ì‚¬ì´ë“œë°” ê³ ë ¤í•œ ìµœëŒ€ ë„ˆë¹„ */
    }
    
    [data-testid="stChatInput"] textarea {
        border: 2px solid #e9ecef !important;
        border-radius: 25px !important;
        padding: 12px 20px !important;
        font-size: 1rem !important;
        resize: none !important;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1) !important;
        transition: all 0.2s ease !important;
        width: 100% !important;
    }
    
    [data-testid="stChatInput"] textarea:focus {
        border-color: #3498db !important;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1), 0 2px 10px rgba(0,0,0,0.15) !important;
        outline: none !important;
    }
    
    [data-testid="stChatInput"] textarea::placeholder {
        color: #7f8c8d !important;
        font-size: 1rem !important;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ì˜ì—­ ì—¬ë°± */
    .element-container:has([data-testid="stChatInput"]) {
        margin-bottom: 80px;
    }
    
    /* ëª¨ë°”ì¼ ëŒ€ì‘ */
    @media (max-width: 768px) {
        .stChatInput {
            left: 0;
        }
        [data-testid="stChatInput"] {
            max-width: 100vw;
        }
    }
</style>
""", unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥ (ì»¤ìŠ¤í…€ placeholder)
if prompt := st.chat_input("AHN'S AI ì—ê²Œ ë¬¼ì–´ë³´ê¸°"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # AI ì‘ë‹µ ìƒì„±
    if api_key:
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = search_documents(
                prompt, 
                st.session_state.documents, 
                st.session_state.embeddings, 
                st.session_state.encoder
            )
            
            # ì‘ë‹µ ìƒì„±
            response = generate_response(prompt, relevant_docs, api_key)
            
            # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥ (ì°¸ê³  ë¬¸ì„œ í¬í•¨)
            message_data = {"role": "assistant", "content": response}
            if relevant_docs:
                message_data["references"] = relevant_docs
            
            st.session_state.messages.append(message_data)
            
        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ UI ì—…ë°ì´íŠ¸
        st.rerun()
    else:
        st.error("ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("System Information"):
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Getting Started:**
        1. Enter your Google Gemini API key in the sidebar
        2. Upload documents using the file uploader
        3. Click "Process Documents" to enable AI search
        4. Ask questions about your documents in the chat
        """)
    
    with col2:
        st.markdown("""
        **Features:**
        - PDF, DOCX, TXT file support
        - Multiple file upload capability
        - Vector-based document search
        - Professional AI responses
        - Source document references
        """)

# í‘¸í„°
st.markdown("---")
st.markdown("**AHN'S AI Assistant** | Enterprise Document Intelligence Platform | Powered by Google Gemini")
